/**
 * LangChain Todo Orchestrator
 * Combines CloudFlare Workers AI, Neon vector store, and GitHub operations
 * into intelligent todo management chains
 */

import { CloudflareWorkersAI } from '@langchain/cloudflare';
import { Neon } from '@langchain/community/vectorstores/neon';
import { LLMChain, RetrievalQAChain, ConversationalRetrievalQAChain } from 'langchain/chains';
import { PromptTemplate } from '@langchain/core/prompts';
import { Document } from '@langchain/core/documents';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

export class TodoLangChainOrchestrator {
  constructor(env) {
    // CloudFlare Workers AI LLM
    this.llm = new CloudflareWorkersAI({
      binding: env.AI,
      model: '@cf/meta/llama-3-8b-instruct',
    });

    // Neon vector store for similarity search
    this.vectorStore = new Neon({
      connectionString: env.NEON_DATABASE_URL,
      tableName: 'todo_embeddings',
      columns: {
        contentColumnName: 'content',
        embeddingColumnName: 'embedding',
        metadataColumnName: 'metadata',
      },
    });

    // CloudFlare Vectorize for fast search
    this.cfVectorize = env.VECTORIZE;

    // GitHub API for repo operations
    this.githubToken = env.GITHUB_TOKEN;
    this.repoOwner = 'chitcommit';
    this.repoName = 'chittychat-data';
  }

  /**
   * Create intelligent chains for todo operations
   */
  async createChains() {
    // Chain 1: Todo Analysis & Categorization
    const analyzeChain = new LLMChain({
      llm: this.llm,
      prompt: PromptTemplate.fromTemplate(`
        Analyze this todo list and:
        1. Identify patterns and themes
        2. Suggest priority order
        3. Detect dependencies between tasks
        4. Recommend task groupings
        5. Estimate complexity (1-5 scale)

        Todos: {todos}

        Output as JSON with structure:
        {{
          "patterns": [],
          "priorities": [],
          "dependencies": [],
          "groups": [],
          "complexity_scores": {{}}
        }}
      `),
    });

    // Chain 2: Conflict Resolution Chain
    const conflictChain = new LLMChain({
      llm: this.llm,
      prompt: PromptTemplate.fromTemplate(`
        Two todo branches have conflicting changes:

        Base Branch: {baseTodos}
        Feature Branch: {featureTodos}

        Suggest a merge strategy that:
        1. Preserves important work from both branches
        2. Eliminates duplicates
        3. Maintains logical task order
        4. Resolves status conflicts

        Output the merged todo list with explanations.
      `),
    });

    // Chain 3: RAG Chain for Context-Aware Suggestions
    const ragChain = ConversationalRetrievalQAChain.fromLLM(
      this.llm,
      this.vectorStore.asRetriever({
        k: 5,
        searchType: 'similarity',
      }),
      {
        qaTemplate: `Use the following context to suggest next todos:
        Context: {context}
        Current Todos: {question}

        Based on similar past sessions and patterns, suggest:
        1. Missing tasks that are usually included
        2. Common next steps after these tasks
        3. Potential blockers to watch for`,

        questionGeneratorTemplate: `Given the conversation and current todos,
        what similar work has been done before?`,
      }
    );

    return { analyzeChain, conflictChain, ragChain };
  }

  /**
   * Process todos through LangChain pipeline
   */
  async processTodos(todos) {
    const chains = await this.createChains();

    // Step 1: Analyze current todos
    const analysis = await chains.analyzeChain.call({
      todos: JSON.stringify(todos),
    });

    // Step 2: Get RAG suggestions based on history
    const suggestions = await chains.ragChain.call({
      question: JSON.stringify(todos),
      chat_history: await this.getChatHistory(),
    });

    // Step 3: Store embeddings for future retrieval
    await this.storeTodoEmbeddings(todos);

    // Step 4: Create GitHub issue with analysis
    await this.createGitHubIssue(analysis, suggestions);

    return { analysis, suggestions };
  }

  /**
   * Store todo embeddings in both Neon and CloudFlare Vectorize
   */
  async storeTodoEmbeddings(todos) {
    // Prepare documents
    const documents = todos.map(todo => new Document({
      pageContent: `${todo.content} - Status: ${todo.status}`,
      metadata: {
        id: todo.id,
        session_id: todo.session_id,
        timestamp: new Date().toISOString(),
        chitty_id: todo.chitty_id || `CT-TODO-${Date.now()}`,
      },
    }));

    // Split if needed
    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });
    const splitDocs = await splitter.splitDocuments(documents);

    // Store in Neon vector store
    await this.vectorStore.addDocuments(splitDocs);

    // Also store in CloudFlare Vectorize for fast retrieval
    const embeddings = await this.generateEmbeddings(splitDocs);
    await this.cfVectorize.upsert(embeddings);
  }

  /**
   * Generate embeddings using CloudFlare AI
   */
  async generateEmbeddings(documents) {
    const embeddings = [];

    for (const doc of documents) {
      const response = await this.llm.binding.run('@cf/baai/bge-large-en-v1.5', {
        text: doc.pageContent,
      });

      embeddings.push({
        id: crypto.randomUUID(),
        values: response.data[0],
        metadata: doc.metadata,
      });
    }

    return embeddings;
  }

  /**
   * Intelligent merge using LangChain
   */
  async mergeTodoBranches(baseBranch, featureBranch) {
    const chains = await this.createChains();

    // Get todos from both branches
    const baseTodos = await this.fetchBranchTodos(baseBranch);
    const featureTodos = await this.fetchBranchTodos(featureBranch);

    // Use conflict resolution chain
    const mergeResult = await chains.conflictChain.call({
      baseTodos: JSON.stringify(baseTodos),
      featureTodos: JSON.stringify(featureTodos),
    });

    // Create PR with merge suggestion
    await this.createPullRequest(baseBranch, featureBranch, mergeResult);

    return mergeResult;
  }

  /**
   * Semantic search across all todos using RAG
   */
  async searchTodos(query) {
    // Search in CloudFlare Vectorize (fast)
    const cfResults = await this.cfVectorize.query({
      vector: await this.getQueryEmbedding(query),
      topK: 10,
    });

    // Search in Neon (comprehensive)
    const neonResults = await this.vectorStore.similaritySearch(query, 10);

    // Combine and deduplicate results
    const combined = this.combineSearchResults(cfResults, neonResults);

    // Use LLM to rank and summarize
    const summary = await this.llm.call({
      prompt: `Summarize these search results for query "${query}": ${JSON.stringify(combined)}`,
    });

    return { results: combined, summary };
  }

  /**
   * Auto-generate todos based on project context
   */
  async generateTodos(projectContext) {
    const prompt = PromptTemplate.fromTemplate(`
      Based on this project context, generate a comprehensive todo list:

      Project: {project}
      Current State: {state}
      Goals: {goals}

      Generate todos that:
      1. Are specific and actionable
      2. Follow logical dependencies
      3. Include success criteria
      4. Have reasonable scope

      Format as JSON array with: content, priority, estimated_hours, dependencies
    `);

    const chain = new LLMChain({ llm: this.llm, prompt });

    const result = await chain.call({
      project: projectContext.name,
      state: projectContext.currentState,
      goals: projectContext.goals,
    });

    // Parse and store generated todos
    const todos = JSON.parse(result.text);
    await this.storeTodoEmbeddings(todos);

    return todos;
  }

  /**
   * Create GitHub issue with LangChain analysis
   */
  async createGitHubIssue(analysis, suggestions) {
    const octokit = new Octokit({ auth: this.githubToken });

    await octokit.rest.issues.create({
      owner: this.repoOwner,
      repo: this.repoName,
      title: `Todo Analysis: ${new Date().toISOString().split('T')[0]}`,
      body: `## AI Analysis\n\n${analysis}\n\n## Suggestions\n\n${suggestions}`,
      labels: ['todo-sync', 'ai-generated'],
    });
  }

  /**
   * GitHub Projects integration with LangChain
   */
  async syncWithGitHubProjects(todos) {
    const octokit = new Octokit({ auth: this.githubToken });

    // Get or create project
    const project = await this.getOrCreateProject(octokit);

    // Create cards for each todo group
    for (const todo of todos) {
      // Use LLM to determine column
      const column = await this.determineProjectColumn(todo);

      await octokit.rest.projects.createCard({
        column_id: column.id,
        note: `${todo.content}\n\nChittyID: ${todo.chitty_id}`,
      });
    }
  }

  /**
   * Monitor and learn from todo patterns
   */
  async learnFromPatterns() {
    // Fetch all historical todos
    const history = await this.vectorStore.similaritySearch('*', 1000);

    // Identify patterns using LLM
    const patterns = await this.llm.call({
      prompt: `Analyze these historical todos and identify:
      1. Common sequences of tasks
      2. Typical time estimates vs actual
      3. Frequent blockers
      4. Success patterns

      Todos: ${JSON.stringify(history)}`,
    });

    // Store patterns for future use
    await this.storePatterns(patterns);

    return patterns;
  }
}

// Export for CloudFlare Worker
export default {
  async fetch(request, env) {
    const orchestrator = new TodoLangChainOrchestrator(env);

    const url = new URL(request.url);

    switch (url.pathname) {
      case '/process':
        const todos = await request.json();
        return Response.json(await orchestrator.processTodos(todos));

      case '/merge':
        const { base, feature } = await request.json();
        return Response.json(await orchestrator.mergeTodoBranches(base, feature));

      case '/search':
        const { query } = await request.json();
        return Response.json(await orchestrator.searchTodos(query));

      case '/generate':
        const context = await request.json();
        return Response.json(await orchestrator.generateTodos(context));

      case '/learn':
        return Response.json(await orchestrator.learnFromPatterns());

      default:
        return new Response('LangChain Todo Orchestrator', { status: 200 });
    }
  }
};